{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis\n",
    "\n",
    "Regression is a type of statistical modeling that allows to investigate whether a variable is dependent on others. The relationship between variables is illustrated by a trend-line which is overlaid on the data and can be used for predicting many different things.\n",
    "\n",
    "<img src=\"res/linear-regression-4.png\" width=\"400\">\n",
    "\n",
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import math\n",
    "from IPython import display\n",
    "\n",
    "from res.plot_lib import plot_data, plot_model, set_default\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "set_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running on a GPU: device string\n",
    "Switching between CPU and GPU in PyTorch is controlled via a device string, which will seemlessly determine whether GPU is available, falling back to CPU if not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the reproducibility, we need to limit the nondeterministic behavior of the platform. We can choose spesific seed to control the randomness (have determinism) over multiple executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the data\n",
    "Let's prepare a <b>one-dimensional float tensor</b> ``X`` of size ``N`` with values evenly spaced from -1 to 1 using ``torch.linspace`` as our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100  # number of samples\n",
    "X = torch.unsqueeze(torch.linspace(-1, 1, N), dim=1).to(device) # training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ground-truth label ``y`` is generated using formula \n",
    "\\begin{equation} y = X^3 + 0.3 * rn \\end{equation} \n",
    "with $rn$ is a random number from a uniform distribution on the interval \\[0,1\\) that can be generated using ``torch.rand``. ``y`` is also a <b> one-dimensional float tensor </b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X.pow(3) + 0.3 * torch.rand(X.size()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shapes of the data:\")\n",
    "print(\"X:\", tuple(X.size()))\n",
    "print(\"y:\", tuple(y.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.cpu().numpy(), y.cpu().numpy())\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above we can roughly see the relationship between x and y. Now we can try drawing a line that can generally describe the relationship between those two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Linear regression models the relationship between the variables using a linear equation, \n",
    "\\begin{equation} Y = a + b X_1 + c X_2 + ... \\end{equation} \n",
    "\n",
    "Let's build a linear model that fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network\n",
    "\n",
    "Now we will make a linear model with one hidden layer using ``nn.linear``. \n",
    "* The input is 1 dimension for training data ``X``. ``X`` is a <b> one-dimensional float tensor </b>.\n",
    "* We use 100 hidden units\n",
    "* The output dimension is also 1 since we want to predict label ``y_pred`` which is also a <b> one-dimensional float tensor </b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 1  # input dimensions\n",
    "C = 1  # output dimensions\n",
    "H = 100  # num_hidden_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "lambda_l2 = 1e-5\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.Linear(H, C)\n",
    ")\n",
    "model.to(device) # Convert to CUDA if exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important arguments to compile are the loss and the optimizer since these define what will be optimized. We can calculate the error for the predicted label ``y_pred`` using using MSE loss for the regression task. As for optimizer, Adam is applied using ``torch.optim`` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "Train the model for 1000 epoch with training data ``X`` and ground truth label ``y`` we created in the beginning. The loss is calculated between ground truth label ``y`` and predicted label ``y_pred``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "for t in range(epochs):\n",
    "    \n",
    "    # Feed forward to get the logits\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    # Compute the loss (MSE)\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(\"[EPOCH]: %i, [LOSS or MSE]: %.6f\" % (t+1, loss.item()))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # zero the gradients before running\n",
    "    # the backward pass.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass to compute the gradient\n",
    "    # of loss w.r.t our learnable params. \n",
    "    loss.backward()\n",
    "    \n",
    "    # Update params\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot\n",
    "Let's see how the nonlinear model fitting the data by plotting the predicted label ``y_pred`` against the training data ``X``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.data.cpu().numpy(), y.data.cpu().numpy())  # training data X - ground truth label y\n",
    "plt.plot(X.data.cpu().numpy(), y_pred.data.cpu().numpy(), 'r-', lw=5) # training data X - predicted label y_pred\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear regression\n",
    "\n",
    "We see that the trained model could not fit the data well. Thus we want to introduce non-linearities to approximate the function. Nonlinear regression can provide flexible curve-fitting functionality. There are several non-linearities that can be used. The most popular ones used for nonlinear regression in neural network architecture is ReLU, TanH, and Sigmoid. \n",
    "\n",
    "<img src=\"res/activation-functions.jpg\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network\n",
    "\n",
    "We will make a nonlinear model with one hidden layer using ``nn.linear``. This time we will add Rectified Linear Unit into the network using ``nn.ReLU``\n",
    "* The input is 1 dimension for training data ``X``. ``X`` is a <b> one-dimensional float tensor </b>.\n",
    "* We use 100 hidden units\n",
    "* The output dimension is also 1 since we want to predict label ``y_pred`` which is also a <b> one-dimensional float tensor </b>.\n",
    "* Apply ``nn.ReLu``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 1  # input dimensions\n",
    "C = 1  # output dimensions\n",
    "H = 100  # num_hidden_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "lambda_l2 = 1e-5\n",
    "\n",
    "model_nl = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.ReLU(), # non-linearity\n",
    "    nn.Linear(H, C)\n",
    ")\n",
    "\n",
    "model_nl.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model_nl.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "Train the model for 1000 epoch with training data ``X`` and ground truth label ``y`` we created in the beginning. The loss is calculated between ground truth label ``y`` and predicted label ``y_pred``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "for t in range(epochs):\n",
    "    \n",
    "    # Feed forward to get the logits\n",
    "    y_pred = model_nl(X)\n",
    "    \n",
    "    # Compute the loss (MSE)\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(\"[EPOCH]: %i, [LOSS or MSE]: %.6f\" % (t+1, loss.item()))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # zero the gradients before running\n",
    "    # the backward pass.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass to compute the gradient\n",
    "    # of loss w.r.t our learnable params. \n",
    "    loss.backward()\n",
    "    \n",
    "    # Update params\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot\n",
    "Let's see how the trained model fitting the data by plotting the predicted label ``y_pred`` against the training data ``X``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X.data.cpu().numpy(), y.data.cpu().numpy()) # training data X - ground truth label y\n",
    "plt.plot(X.data.cpu().numpy(), y_pred.data.cpu().numpy(), 'r-', lw=5) # training data X - predicted label y_pred\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the nonlinear model fits the data better compared to the linear model.\n",
    "\n",
    "# Hands-on Exercises\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "In this section we will try using another non-linearity: hyperbolic tanget (tanh) function.\n",
    "* Define a new neural network with tanh nonlinearity using ``nn.Tanh()``\n",
    "* Train the model with data ``X`` and ``y`` we created in the beginning for 1000 epochs\n",
    "* Will it be different compared to when using ReLu as the nonlinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network\n",
    "\n",
    "We will make a nonlinear model with one hidden layer using ``nn.linear``. We will add hyperbolic tangent function into the network using ``nn.Tanh``\n",
    "* The input is 1 dimension for training data ``X``. ``X`` is a <b> one-dimensional float tensor </b>.\n",
    "* We use 100 hidden units\n",
    "* The output dimension is also 1 since we want to predict label ``y_pred`` which is also a <b> one-dimensional float tensor </b>.\n",
    "* Apply ``nn.Tanh``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 1  # input dimensions\n",
    "C = 1  # output dimensions\n",
    "H = 100  # num_hidden_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "lambda_l2 = 1e-5\n",
    "\n",
    "# define a network with tanh nonlinearity\n",
    "model_nl2 = nn.Sequential(\n",
    "    # put your code here\n",
    ")\n",
    "\n",
    "model_nl2.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model_nl2.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Train for 1000 epochs with training data ``X`` and ground truth label ``y`` we created in the beginning. The loss is calculated between ground truth label ``y`` and predicted label ``y_pred``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "for t in range(epochs):\n",
    "    \n",
    "    # Feed forward to get the logits\n",
    "    y_pred = model_nl2(X)\n",
    "    \n",
    "    # Compute the loss (MSE)\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(\"[EPOCH]: %i, [LOSS or MSE]: %.6f\" % (t+1, loss.item()))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # zero the gradients before running\n",
    "    # the backward pass.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass to compute the gradient\n",
    "    # of loss w.r.t our learnable params. \n",
    "    loss.backward()\n",
    "    \n",
    "    # Update params\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_nl2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot\n",
    "Let's see how the trained model with tanh nonlinearity fits the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.data.cpu().numpy(), y.data.cpu().numpy())\n",
    "plt.plot(X.data.cpu().numpy(), y_pred.data.cpu().numpy(), 'r-', lw=5)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "In this section we will create a new dataset and train a nonlinear model to fit it.\n",
    "* Create a new training data ``X_2`` with only 10 samples with values evenly spaced from 0 to 1 using ``torch.linspace``\n",
    "* Create a new ground-truth label ``y_2`` using formula \n",
    "    \\begin{equation} y = X^3 - 0.25 * rn \\end{equation} \n",
    "  with $rn$ is a random number from a uniform distribution on the interval \\[0,1\\) that can be generated using ``torch.rand``.\n",
    "* Train a nonlinear model with ReLu to fit the new dataset\n",
    "* Does it fit well? What if the nonlinearity is changed to tanh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of samples here\n",
    "_N = ...\n",
    "\n",
    "# Create training data X_2\n",
    "X_2 = torch.unsqueeze(torch.linspace(0, 1, _N), dim=1).to(device)\n",
    "\n",
    "# Create a ground truth label y_2\n",
    "y_2 = X_2.pow(3) - 0.25 * torch.rand(X_2.size()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how the new dataset looks like\n",
    "plt.scatter(X_2.cpu().numpy(), y_2.cpu().numpy())\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network\n",
    "\n",
    "We will make a nonlinear model with one hidden layer using ``nn.linear``. This time we will add Rectified Linear Unit into the network using ``nn.ReLU``\n",
    "* The input is 1 dimension for the newly created training data ``X_2``. ``X_2`` is a <b> one-dimensional float tensor </b>.\n",
    "* We use 100 hidden units\n",
    "* The output dimension is also 1 since we want to predict label ``y_2pred`` which is also a <b> one-dimensional float tensor </b>.\n",
    "* Apply ``nn.ReLu``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 1  # input dimensions\n",
    "C = 1  # output dimensions\n",
    "H = 100  # num_hidden_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "lambda_l2 = 1e-5\n",
    "\n",
    "model_nl = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.ReLU(), # non-linearity\n",
    "    nn.Linear(H, C)\n",
    ")\n",
    "\n",
    "model_nl.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model_nl.parameters(), lr=learning_rate, weight_decay=lambda_l2) # built-in L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "Train the model for 1000 epoch for the newly created training data ``X_2`` and ground truth label ``y_2``. The loss is calculated between ground truth label ``y_2`` and predicted label ``y_2pred``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "for t in range(epochs):\n",
    "    \n",
    "    # Feed forward to get the logits\n",
    "    y_2pred = model_nl(X_2)\n",
    "    \n",
    "    # Compute the loss (MSE)\n",
    "    loss = criterion(y_2pred, y_2)\n",
    "    print(\"[EPOCH]: %i, [LOSS or MSE]: %.6f\" % (t+1, loss.item()))\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # zero the gradients before running\n",
    "    # the backward pass.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass to compute the gradient\n",
    "    # of loss w.r.t our learnable params. \n",
    "    loss.backward()\n",
    "    \n",
    "    # Update params\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot\n",
    "Let's see how the trained model with ReLu nonlinearity fits the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_2.data.cpu().numpy(), y_2.data.cpu().numpy()) # training data X_2 - ground truth label y_2\n",
    "plt.plot(X_2.data.cpu().numpy(), y_2pred.data.cpu().numpy(), 'r-', lw=5) # training data X_2 - predicted label y_2pred\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next tutorial: Training a classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
