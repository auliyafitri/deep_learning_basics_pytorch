{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch - Part I\n",
    "This notebook forms Part I of the practical examples and exercises accompanying the Seminar \"Introduction to Deep Learning\".<br><br>\n",
    "This part gives an introduction into deep learning using the PyTorch framework.  \n",
    "Any references in this notebook refer to the slides received during the seminar.  \n",
    "\n",
    "\n",
    "For any claims and suggestions please refer to\n",
    "* Jakob Gawlikowski (jakob.gawlikowski@dlr.de)\n",
    "* Auliya Fitri (auliya.fitri@dlr.de)\n",
    "<br><br>\n",
    "\n",
    "***\n",
    "**Part I** *(this notebook)*\n",
    "\n",
    "| **Content Part I**         | \n",
    "| :-----------:              |\n",
    "| 1) **Python and PyTorch**      | \n",
    "| 2) **Single Neuron**           |\n",
    "| 3) **Activation functions**    |\n",
    "| 4) **Single Neuron Regression & Classification** |\n",
    "| 5) **Loss Functions**                            |\n",
    "| 6) **Gradients and Backward Propagation**        |\n",
    "| 7) **Optimizer**                               |\n",
    "\n",
    "**Part II** *(in the afternoon)*:\n",
    "\n",
    "| **Content Part II**   \n",
    "| :-----------:              | \n",
    "|1) **Practical examples and excercises for training of neural networks** |\n",
    "|2) **Regression Neural Networks** |\n",
    "|3) **Classification Neural Network** |\n",
    "|4) **Convolutional Neural Networks for image data** |\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The PyTorch Library\n",
    "PyTorch is a Deep Learning Library that offers a lot of implemented functionalites. Those functionalities are seperated into three modules:\n",
    "* building a neural network: ```torch.nn```\n",
    "* optimization: ```torch.optim```\n",
    "* automatic differentiation: ```torch.autograd```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import needed packages into Python Environment**  \n",
    "In order to work with PyTorch in our python environent, we have to import the PyTorch package which is called <i>torch</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pytorch Package\n",
    "import torch\n",
    "from torch import nn, tensor   \n",
    "\n",
    "# Packages for Data Generation and Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# customn functions for simple processing\n",
    "from utils.printer import print_function    # plots a given function for a given range\n",
    "from utils.printer import print_classification_regions\n",
    "from utils.data_helper import get_cluster_data_set\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is build up on so-called *tensors*, which can be of any dimension. For these tensors PyTorch offers multiple pre-implemented functionalities, including basic calculus. The functionalities can be applied on the whole tensor (e.g., vector multiplication) or elementwise (e.g. sums and differences):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([3,2,1])\n",
    "\n",
    "\n",
    "\n",
    "print(\"a:     \", a)\n",
    "print(\"b:     \", b)\n",
    "print()\n",
    "print(\"Shape of a:     \", a.shape)\n",
    "print(\"Shape of b:     \", b.shape)\n",
    "print()\n",
    "\n",
    "a_plus_b = a+b\n",
    "print(\"Addition        (a+b):   \", a_plus_b)\n",
    "\n",
    "a_minus_b = a-b\n",
    "print(\"Subtraction     (a-b):   \", a_minus_b)\n",
    "\n",
    "a_divided_by_b = a/b\n",
    "print(\"Division        (a/b):   \", a_divided_by_b)\n",
    "\n",
    "a_times_b = a*b\n",
    "print(\"Multiplication  (a*b):   \", a_times_b)\n",
    "\n",
    "a_to_the_power_of_b = a**b\n",
    "print(\"Exponentials    (a**b):  \", a_to_the_power_of_b)\n",
    "\n",
    "inner_product = a@b\n",
    "print(\"Inner product   (a@b):   \", inner_product)\n",
    "print(\"... \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything works the same way for two-dimensional tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3],[0,0,0]])\n",
    "b = torch.tensor([[3,2,1], [3,2,1]])\n",
    "print(\"a:     \", str(a).replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "print(\"b:     \", str(b).replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "print()\n",
    "print(\"Shape of a:     \", a.shape)\n",
    "print(\"Shape of b:     \", b.shape)\n",
    "print()\n",
    "a_plus_b = a+b\n",
    "print(\"Addition        (a+b):   \", str(a_plus_b).replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "\n",
    "a_minus_b = a-b\n",
    "print(\"Subtraction     (a-b):   \", str(a_minus_b).replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "\n",
    "a_divided_by_b = a/b\n",
    "print(\"Division        (a/b):   \", str(a_divided_by_b).replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "\n",
    "a_times_b = a*b\n",
    "print(\"Multiplication  (a*b):   \", str(a_times_b).replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "\n",
    "a_to_the_power_of_b = a**b\n",
    "print(\"Exponentials    (a**b):  \", str(a_to_the_power_of_b).replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "\n",
    "a_transposed = a.T\n",
    "print(\"Transpose       (a.T):   \", str(a.T).replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "\n",
    "matrix_product = a.T@b\n",
    "print(\"Matrix product  (a.T@b): \", str(a.T@b).replace(\"\\n\", \"\").replace(\" \", \"\"))\n",
    "print(\"... \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A Single Neuron\n",
    " <img src=\"res/single_neuron.png\" width=\"400\">\n",
    "\n",
    "PyTorch offers implementations of basic neural network layers and hence single neurons do not have to be implemented by hand.  \n",
    "The following part is only for a better general understanding of the concepts of Neural Networks.  \n",
    "For this we consider inputs $x\\in\\mathbb{R}^4$ and weights $w\\in\\mathbb{R}^4$.  \n",
    "\n",
    "We consider two diferent inputs \n",
    "$$x_1=\\begin{pmatrix}1\\\\2\\\\3\\\\4\\end{pmatrix}\\in \\mathbb{R}^4 \\qquad x_2=\\begin{pmatrix}1\\\\1\\\\1\\\\1\\end{pmatrix}\\in \\mathbb{R}^4$$\n",
    "and two different weights\n",
    "$$w_1=\\begin{pmatrix}-1\\\\~~~1\\\\-1\\\\~~~1\\end{pmatrix}\\in \\mathbb{R}^4 \\qquad w_2=\\begin{pmatrix}1\\\\1\\\\1\\\\1\\end{pmatrix}\\in \\mathbb{R}^4 \\qquad .$$\n",
    "As stated in Slide 33, data is processed by the neuron in the following way:\n",
    "$$\\begin{align*}\n",
    "&x_1^Tw_1 =&-1+2-3+4 &=& 2\\\\ \n",
    "&x_2^Tw_1 =&1-1+1-1 &=& 0\\\\\n",
    "&x_1^Tw_2 =&1+2+3+4 &=& 10\\\\ \n",
    "&x_2^Tw_2 =&1+1+1+1 &=& 4\n",
    "\\end{align*}$$\n",
    "\n",
    "Using PyTorch the matrix multiplication can either be realized with the ```torch.matmul``` function or in a more compact way by using the operator @."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weights without bias value\n",
    "w1 = tensor([-1, 1, -1, 1], dtype=torch.float32)\n",
    "w2 = tensor([1, 1, 1, 1], dtype=torch.float32)\n",
    "\n",
    "# x values\n",
    "x1 = tensor([1,2,3,4], dtype=torch.float32)\n",
    "x2 = tensor([1,1,1,1], dtype=torch.float32)\n",
    "\n",
    "# Vector / Matrix multiplication\n",
    "y11=x1@w1.T\n",
    "y21=x2@w1.T\n",
    "y12=x1@w2.T\n",
    "y22=x2@w2.T\n",
    "\n",
    "print(f\"Output for input 1 and weights 1: {y11.item()}\")\n",
    "print(f\"Output for input 2 and weights 1: {y21.item()}\")\n",
    "print(\"\")\n",
    "print(f\"Output for input 1 and weights 2: {y12.item()}\")\n",
    "print(f\"Output for input 2 and weights 2: {y22.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated on Slide 34 we can also add bias term, which is added to the result. Let's consider the terms $b_1=1~$ and $~b_2=-4~$ leads to the following results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1=1\n",
    "b2=-4\n",
    "\n",
    "print((x1@w1 + b1).item())\n",
    "print((x2@w1 + b1).item())\n",
    "print((x1@w2 + b2).item())\n",
    "print((x2@w2 + b2).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res/single_neuron_bias.png\" width=\"400\">\n",
    "As stated on slide 35 the bias term can be included into the weight vector by adding a $1$ to the inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights without bias value\n",
    "w1 = torch.tensor([-1, 1, -1, 1, b1], dtype=torch.float32)\n",
    "w2 = torch.tensor([1, 1, 1, 1, b2], dtype=torch.float32)\n",
    "\n",
    "# Itorch.uts\n",
    "x1 = torch.tensor([1, 2, 3, 4, 1], dtype=torch.float32)\n",
    "x2 = torch.tensor([4, 3, 2, 1, 1], dtype=torch.float32)\n",
    "\n",
    "# do vector multiplication of itorch.ut vector and (transposed) weight vector\n",
    "y11=x1@w1.T\n",
    "y21=x2@w1.T\n",
    "y12=x1@w2.T\n",
    "y22=x2@w2.T\n",
    "\n",
    "print(y11.item())\n",
    "print(y21.item())\n",
    "print(y12.item())\n",
    "print(y22.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Activation Functions / Non-Linearities\n",
    "In PyTorch the most common activation functions are pre-installed and can be found in <b><i>torch.nn</i></b> . \n",
    "\n",
    "Popular functions are for example given by <br>\n",
    "<code>\n",
    "&emsp;&emsp;relu = nn.ReLU()<br>\n",
    "&emsp;&emsp;leaky_relu = nn.LeakyReLU(0.1)<br>\n",
    "&emsp;&emsp;sigmoid = nn.Sigmoid()<br>\n",
    "&emsp;&emsp;tanh = nn.Tanh()<br>\n",
    "</code>\n",
    "\n",
    "\n",
    "An overview over all pre-implemented activations cna be found here:<br>\n",
    "* https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity <br>\n",
    "* https://pytorch.org/docs/stable/nn.html#non-linear-activations-other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ploting popular Activation Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.ReLU()\n",
    "leaky_relu = nn.LeakyReLU(0.1)\n",
    "sigmoid = nn.Sigmoid()\n",
    "tanh = nn.Tanh()\n",
    "    \n",
    "print_function(relu, name=\"ReLU\", color=\"red\", title=\"Activation Functions\")\n",
    "print_function(leaky_relu, name=\"Leaky ReLU\", color=\"blue\", title=\"Activation Functions\")\n",
    "print_function(tanh, name=\"Tanh\", color=\"black\", title=\"Activation Functions\")\n",
    "print_function(sigmoid, name=\"Sigmoid\", color=\"green\", title=\"Activation Functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying these activation functions to our neurons defined above leads to the following outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"|  Weights 1 |        || ReLU     | Leaky ReLU            | Sigmoid                     |Tanh          \")\n",
    "print(f\"|------------|--------------------------------------------------------------------------------------------------------\")\n",
    "print(f\"|  Output 1: |{y11:<8}|| {relu(y11):<8} | {leaky_relu(y11).item():<20}  | {sigmoid(y11):<20}        | {tanh(y11):<20} \")\n",
    "print(f\"|  Output 2: |{y21:<8}|| {relu(y21):<8} | {leaky_relu(y21).item():<20}  | {sigmoid(y21):<20}        | {tanh(y21):<20} \")\n",
    "print()\n",
    "print(f\"|  Weights 2 |        || ReLU     | Leaky ReLU            | Sigmoid                     |Tanh          \")\n",
    "print(f\"|------------|--------------------------------------------------------------------------------------------------------\")\n",
    "print(f\"|  Output 1: |{y12:<8}|| {relu(y12):<8} | {leaky_relu(y12).item():<20}  | {sigmoid(y12):<20}        | {tanh(y12):<20} \")\n",
    "print(f\"|  Output 2: |{y22:<8}|| {relu(y22):<8} | {leaky_relu(y22).item():<20}  | {sigmoid(y22):<20}        | {tanh(y22):<20} \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pytorch operators can not only be applied to scalers, but also to tensors in a point-wise way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.stack([x1,x2])\n",
    "w = torch.stack([w1, w2])\n",
    "y = x@w.t()\n",
    "\n",
    "print(\"y tensor:\\n\",y)\n",
    "print()\n",
    "print(\"ReLU activation:\\n\", relu(y))\n",
    "print(\"Sigmoid activation:\\n\", sigmoid(y))\n",
    "print(\"Tanh activation:\\n\", tanh(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 'Neural Networks' with one Neuron  \n",
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the data**  \n",
    "Let's prepare a <b>one-dimensional float tensor</b> ``X`` of size ``N`` with values evenly spaced from -1 to 1 using ``torch.linspace`` as our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 150  # number of samples\n",
    "X_reg = torch.unsqueeze(torch.linspace(-1, 1, N), dim=1) # training data\n",
    "y = X_reg**3 + 0.1 * torch.normal(0,1,X_reg.size())\n",
    "\n",
    "print(\"Shapes of the data:\")\n",
    "print(\"X:\", tuple(X_reg.size()))\n",
    "print(\"y:\", tuple(y.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_reg.cpu().numpy(), y.cpu().numpy())\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Approximation**  \n",
    "We want to approximate the function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ by single neuron. Hence, the regression model is of the form\n",
    "$$f(x)=w\\cdot x +b\\quad,$$\n",
    "where $w\\in\\mathbb{R}$ and $b\\in\\mathbb{R}$ are the learnable model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_reg, y, c=\"green\")\n",
    "\n",
    "b = 0\n",
    "w = 0.25\n",
    "f = lambda x: w*x + b\n",
    "plt.plot(X_reg, f(X_reg), c=\"red\", label=f\"w={w}, b={b}\")\n",
    "\n",
    "b = 0\n",
    "w = 1\n",
    "f = lambda x: w*x + b\n",
    "plt.plot(X_reg, f(X_reg), c=\"black\", label=f\"w={w}, b={b}\")\n",
    "\n",
    "b = 0.25\n",
    "w = -0.25\n",
    "f = lambda x: w*x + b\n",
    "plt.plot(X_reg, f(X_reg), c=\"blue\", label=f\"w={w}, b={b}\")\n",
    "\n",
    "plt.title(\"Linear Approximations\")\n",
    "plt.legend()\n",
    "plt.axis('equal');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-Linear Approximation**  \n",
    "We want to approximate the function $f:\\mathbb{R}\\rightarrow\\mathbb{R}$ by single neuron followed by an *activation function*. Hence, the regression model is of the form\n",
    "$$f(x)=\\sigma (w\\cdot x +b)\\quad,$$\n",
    "where $w\\in\\mathbb{R}$ and $b\\in\\mathbb{R}$ are the learnable model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_reg, y, c=\"green\")\n",
    "\n",
    "relu = nn.ReLU()\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "b = -4\n",
    "w = 5\n",
    "f = lambda x: w*x + b\n",
    "plt.plot(X_reg, sigmoid(f(X_reg)), c=\"black\", label=f\"sigmoid, w={w}, b={b}\")\n",
    "\n",
    "b = -0.4\n",
    "w = 1.1\n",
    "f = lambda x: w*x + b\n",
    "plt.plot(X_reg, relu(f(X_reg)), c=\"red\", label=f\"relu, w={w}, b={b}\")\n",
    "\n",
    "plt.legend()\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "Let's get some two-dimensional data set laying in the interval ```[-8,8]x[-8,8]``` and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_class, Y_class = get_cluster_data_set()\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(5, 5))\n",
    "print_classification_regions(X_class, Y_class, ax=ax);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Linear Classifiers\n",
    "\n",
    "Linear classifiers on the 2-dimensional plane can be described by a function of the form $$f(x_1,x_2)=w_1\\cdot x_1 + w_2\\cdot x_2 + b,$$\n",
    "where $w_1,w_2$ and $b$ are the parameters that describe the function. <br><br>\n",
    "The classification is then realized by\n",
    "$$\\text{prediction}~~y=\\begin{cases}\\text{Class 1} \\quad \\text{if}\\quad f(x_1,x_2)<0 \\\\ \\text{Class 2} \\quad \\text{if}\\quad f(x_1,x_2)>0\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "w = torch.tensor([0.25,2])\n",
    "b = 5\n",
    "print_classification_regions(X_class, Y_class, f=lambda x: w@x +b, ax=ax1, title=f\"$f(x)={w[0].item()}\\cdot x_1+{w[1].item()}\\cdot x_2+{b}$\");\n",
    "\n",
    "w = torch.tensor([-1.,1.])\n",
    "b = 0\n",
    "print_classification_regions(X_class, Y_class, f=lambda x: w@x +b, ax=ax2, title=f\"$f(x)={w[0].item()}\\cdot x_1+{w[1].item()}\\cdot x_2+{b}$\");\n",
    "\n",
    "w = torch.tensor([1.,1.])\n",
    "b = -8\n",
    "print_classification_regions(X_class, Y_class, f=lambda x: w@x +b, ax=ax3, title=f\"$f(x)={w[0].item()}\\cdot x_1+{w[1].item()}\\cdot x_2+{b}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss Functions\n",
    "PyTorch comes with a wide range of loss functions. The loss functions are part of the neural network component ```torch.nn```. <br><br>\n",
    "An overview over all available loss functions can be found here: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "<br><br>\n",
    "The maybe most basic loss functions are the\n",
    "* Mean-Sqaured-Error: ```torch.nn.MSELoss()```\n",
    "* Cross-Entropy-Loss: ```torch.nn.CrossEntropy()```\n",
    "*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1,figsize=(15,5))\n",
    "\n",
    "mse_loss = torch.nn.MSELoss(reduce=False)\n",
    "\n",
    "x_in = torch.range(-2,2,0.01)\n",
    "y_target = lambda x: x**2\n",
    "y_pred = lambda x: x+4\n",
    "\n",
    "loss = mse_loss(target=y_target(x_in), input=y_pred(x_in))\n",
    "\n",
    "print_function(y_target, ax=ax1, name=\"Groundtruth\", color=\"green\")\n",
    "ax1.plot(torch.range(-2,2,0.25), loss[::25], \"x\", color=\"red\", label=\"Squared-Error\");\n",
    "print_function(y_pred, ax=ax1, name=\"Prediction\", color=\"blue\", title=\"Linear Approximation of $f(x)=x^2$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradients & Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradients**<br>\n",
    "Pytorch offers automatic differentiation of tensors. For that the parameter ```requires_gradient``` has to be set to ```True```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "print_function(f, title=\"Funtion $f(x)=x^2$\")\n",
    "\n",
    "x1 = torch.tensor(1.0, requires_grad=True)\n",
    "x2 = torch.tensor(-2.0, requires_grad=True)\n",
    "\n",
    "f_x1 = f(x1)\n",
    "f_x1.backward()\n",
    "print(\"Gradient of $f(x_1)$: \", x1.grad)\n",
    "plt.annotate(\"Grad: \" + str(x1.grad), xy=(x1,f(x1)), xytext=(x1+0.2,f(x1)+0.4), fontsize=12)\n",
    "plt.arrow(x1.detach(), f(x1).detach(), 0.5*torch.sign(x1.grad.detach()), 0, head_width = 0.3);\n",
    "\n",
    "f_x2 = f(x2)\n",
    "f_x2.backward()\n",
    "print(\"Gradient of $f(x_2)$: \", x2.grad)\n",
    "plt.annotate(\"Grad: \" + str(x2.grad), xy=(x1,f(x1)), xytext=(x2+0.2,f(x2)+0.4), fontsize=12)\n",
    "plt.arrow(x2.detach(), f(x2).detach(), 0.5*torch.sign(x2.grad.detach()), 0, head_width = 0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimization\n",
    "The core of the optimization procedure in PyTorch is placed in the ```torch.optim``` module: https://pytorch.org/docs/stable/optim.html\n",
    "<br><br>\n",
    "It contains different types of optimization algorithms which can be used out-of-the-box to train a neural network. The most commonly used optimizers are\n",
    "* Adam Algorithm (```torch.optim.Adam```)\n",
    "* Stochastic Gradient Descent (```torch.optim.SGD```)\n",
    "\n",
    "A list of all available optimizers can be found here: <br>\n",
    "https://pytorch.org/docs/stable/optim.html#algorithms\n",
    "<br><br>\n",
    "#### The Training Loop\n",
    "For the training of a neural network a training loop has to be implemented. <br><br>The training loop iterates multiple times over the training data and updates the network parameters in order to better fit the predictions to the training data. \n",
    "The training loop contains the following parts:\n",
    "\n",
    "| **The Training Loop**      | \n",
    "| :-----------:              |\n",
    "| 1) Outer and inner loops that iterate over epochs and batches                 |\n",
    "| <br>```for x,y in data_set:```                                                |\n",
    "| 2) Set accumulated gradients to zero <br>```optimizer.zero_grad()```          |\n",
    "| 3) Propagte sample through the network <br>```y_pred=model(x)```              |\n",
    "| 4) Compute the loss <br>```loss = loss_function(y_true, y_pred)```            |\n",
    "| 5) Backpropagate the gradients <br>```loss.backward()```                      |\n",
    "| 6) Do an update step based on the chosen optimizer <br>```optimizer.step()``` |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data Set and visualization\n",
    "N = 150  # number of samples\n",
    "X_reg = torch.unsqueeze(torch.linspace(-1, 1, N), dim=1)\n",
    "y = X_reg.pow(3) + 0.1 * torch.normal(0,1,X_reg.size())\n",
    "\n",
    "# 'Network' function\n",
    "network = lambda x: w_para * x + b_para\n",
    "# Initial value\n",
    "w_para = torch.tensor(-1, requires_grad=True, dtype=float)\n",
    "b_para = torch.tensor(-1, requires_grad=True, dtype=float)\n",
    "\n",
    "# Learning rate / Step-size for Optimization\n",
    "learning_rate = 0.1\n",
    "# Chose *Stochastic Gradient Descent* algorithm as optimizer\n",
    "optimizer = torch.optim.SGD(params=[w_para,b_para], lr=learning_rate)\n",
    "\n",
    "# Number of optimiztion steps\n",
    "steps = 30\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(3,10, figsize=(20,8))\n",
    "###################################\n",
    "###### EXAMPLE TRAINING LOOP ######\n",
    "###################################\n",
    "pred = network(X_reg)\n",
    "\n",
    "# Step 0: Loop over number of steps (=epochs)\n",
    "for i in range(steps):\n",
    "\n",
    "    ax[i//10,i%10].scatter(X_reg.detach(), y.detach())\n",
    "\n",
    "    # Step 2: Set accumulated gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Step 3: Propagate data through the 'network'\n",
    "    pred = network(X_reg)\n",
    "\n",
    "    # Step 4: Compute loss value\n",
    "    loss_value = loss_fn(pred, y)\n",
    "    x_from = X_reg.detach()\n",
    "\n",
    "    # Step 5: Backpropagate Gradients\n",
    "    loss_value.backward()\n",
    "\n",
    "    # Step 6: Do optimization step\n",
    "    optimizer.step()\n",
    "\n",
    "    x_to = X_reg.detach()\n",
    "    ax[i//10,i%10].set_title(f\"Step {i+1} - Loss: %.3f\" %loss_value.detach(), fontsize=9)\n",
    "    ax[i//10,i%10].plot(torch.linspace(-1,1,600), network(torch.linspace(-1,1,600)).detach(), \"red\", label=\"$f_{w_{\" + str(i) + \"} }(x)$\")\n",
    "    ax[i//10,i%10].legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Own Gradien Descent\n",
    "This section forms a small add-on to visualize the working principles of optimization algorithms as the gradient descent.\n",
    "\n",
    "The general idea behind iterative first order optimization procedures can be visualized by considering the *gradient descent* algorithm. For that consider the following notation: \n",
    "\n",
    "* $(x,y)\\qquad~~$ pairs of input samples $x$ and corresponding labels $y$. \n",
    "* $f_w(x)\\qquad~$ a neural network, parameterized with parameters $w$.\n",
    "* $w\\qquad\\qquad$ the parameteres that should be adjusted during the training process.\n",
    "* $w_k\\qquad~~~~~~$ the model parameters at training iteration $k$.\n",
    "* $\\hat y=f_w(x)~~$ the neural network's prediction for input $x$.\n",
    "* $\\mathcal{L}(\\hat y, y)\\qquad$ the loss function evaluated for a given prediction $\\hat y$ and the groundtruth $y$.\n",
    "* $\\nabla_w f_{w}(x)~~~~$ the gradient with respect to $w$.\n",
    "\n",
    "The gradient descent algorithm iteratively makes steps in the optisite direction of the gradient, e.g., $-\\nabla_\\theta \\mathcal{L}(f(x),y)$. For a chosen step-size, or learning-rate $\\alpha>0$ the update is given by $$w_{k+1} = \\theta_k-\\alpha\\cdot\\nabla_w \\mathcal{L}\\left(f_{w_k}(x),y\\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_init = 6\n",
    "\n",
    "learning_rate = 0.75\n",
    "f = lambda x: x**2\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(10,5))\n",
    "\n",
    "x = torch.tensor(x_init, requires_grad=True, dtype=float)  \n",
    "print_function(f, title=\"Gradient Descent\", start=-x.item()* 1.1, end=x.item()*1.1, ax=ax1)\n",
    "for i in range(steps):\n",
    "    x.requires_grad = True\n",
    "    y = f(x)\n",
    "    ax1.plot(x.detach(), y.detach(), \"o\")\n",
    "    y.backward()\n",
    "    ax1.arrow(x.detach(), f(x).detach(), -learning_rate * x.grad, 0, head_width = 0.3, length_includes_head=True);\n",
    "    x = (x.detach() - learning_rate * x.grad.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "210ad4c248d57536180bb83e7552c6b62837aba485eb29c720d2bd58e31b798b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
